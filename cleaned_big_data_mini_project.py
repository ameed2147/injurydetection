#!/usr/bin/env python
# coding: utf-8



import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import sklearn
from platform import python_version
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier
from sklearn.svm import NuSVC
from sklearn.tree import ExtraTreeClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score, recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, roc_auc_score




# Ensure LightGBM is installed\!pip install lightgbm




from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score




df = pd.read_csv('injury_data.csv')




df.head()




#rounding the values to required decimal points
df['Player_Weight'] = df['Player_Weight'].round(2)
df['Player_Height'] = df['Player_Height'].round(2)
df['Training_Intensity'] = df['Training_Intensity'].round(2)




df.head()




# checkling for null values
df.info()




#creating new columns with derived values
#1.1 bmi - body mass index
df['BMI'] = df['Player_Weight'] / (df['Player_Height'] / 100) ** 2




df.head()




#1.3 bmi category
gaps = [-float('inf'), 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')]
categories = ['Underweight', 'Normal', 'Overweight', 'Obesity I', 'Obesity II', 'Obesity III']
df['BMI_Classification'] = pd.cut(df['BMI'], bins=gaps, labels=categories, right=False)




df.head()




#1.3 age groups
print('Player Age Min: {}'.format(df.Player_Age.min()))
print('Player Age Max: {}'.format(df.Player_Age.max()))




df["Age_Group"] = pd.cut(
    df["Player_Age"],
    bins=[18, 22, 26, 30, 34, df["Player_Age"].max()],
    labels=["18-22", "23-26", "27-30", "31-34", "35+"],
    include_lowest=True,
)

df.head()




import matplotlib.pyplot as plt

# Count the number of players in each age group
age_group_counts = df['Age_Group'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(age_group_counts, labels=age_group_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Players Across Age Groups')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()




import matplotlib.pyplot as plt

# Count the number of players in each age group
age_group_counts = df['BMI_Classification'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(age_group_counts, labels=age_group_counts.index, autopct='%1.1f%%', startangle=300, )
plt.title('Distribution of Players Across Age Groups')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()




one_hot_cols = [
    "BMI_Classification",
    "Age_Group",
]

# Selecting only categorical columns from the DataFrame
df_categorical = df[one_hot_cols]

# Applying OneHotEncoder
encoder = OneHotEncoder()
encoded_data = encoder.fit_transform(df_categorical)

# Obtaining names of the features generated by OneHotEncoder
one_hot_feature_names = encoder.get_feature_names()

# Creating a DataFrame with transformed features
df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_feature_names)

# Joining DataFrames
df_final = pd.concat([df, df_encoded], axis=1)

# Dropping categorical columns
df_final.drop(columns=one_hot_cols, inplace=True)

# Visualizing the first few rows of the final DataFrame
df_final.head()




correlation_matrix = df_final.corr()




plt.figure(figsize=(15, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap of Correlation Matrix', weight='bold', size=13)
plt.show()




# Calculating correlation matrix | Calcular a matriz de correlação
correlation_matrix = df_final.corr()

# Selecting only 'Likelihood_of_Injury' column from the correlation matrix | Selecionar apenas a coluna 'Likelihood_of_Injury' da matriz de correlação
correlation_with_likelihood = correlation_matrix['Likelihood_of_Injury']

# Removing the correlation with the 'Likelihood_of_Injury' column | Remover a correlação com a coluna 'Likelihood_of_Injury'
correlation_with_likelihood = correlation_with_likelihood.drop('Likelihood_of_Injury')

# Sorting correlations in descending order | Ordenar as correlações em ordem decrescente
correlation_with_likelihood = correlation_with_likelihood.sort_values(ascending=False)

# Plotting correlation bar plot | Plotar o gráfico de barras de correlação
plt.figure(figsize=(10, 6))
sns.barplot(x=correlation_with_likelihood.index, y=correlation_with_likelihood.values, palette='coolwarm')
plt.xticks(rotation=90, ha='center')  
plt.xlabel('')
plt.ylabel('')
plt.box(False) 
plt.title('Correlation of Columns with Likelihood_of_Injury', weight='bold', size=13)
plt.tight_layout()
plt.show()




df_final = df_final.loc[:, ~df_final.columns.str.startswith('Age_Group')]

df_final = df_final.drop(columns=['BMI'])

df_final.head(1)




X = df_final.drop('Likelihood_of_Injury', axis=1)

y = df_final['Likelihood_of_Injury']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)




from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score




from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC, NuSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier

# Define the models dictionary
models = {
    "LGBMClassifier": LGBMClassifier(random_state=42),
    "AdaBoostClassifier": AdaBoostClassifier(random_state=42),
    "ExtraTreesClassifier": ExtraTreesClassifier(random_state=42),
    "NuSVC": NuSVC(probability=True),
    "ExtraTreeClassifier": ExtraTreeClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(probability=True),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier(random_state=42)
}

# Loop through each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    predictions = model.predict(X_test)
    
    # Calculate evaluation metrics
    recall = recall_score(y_test, predictions, average='weighted')
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted')
    f1 = f1_score(y_test, predictions, average='weighted')
    
    # Check if the model has predict_proba method
    if hasattr(model, 'predict_proba'):
        probabilities = model.predict_proba(X_test)
        if len(probabilities[0]) > 2:
            roc_auc = roc_auc_score(y_test, probabilities, average='weighted', multi_class='ovr')
        else:
            roc_auc = roc_auc_score(y_test, probabilities[:, 1], average='weighted')
        print(f"ROC AUC Score: {roc_auc}")
    else:
        print("ROC AUC Score: Not available for this model")
    
    # Print evaluation metrics
    print(f"Model: {model_name}")
    print(f"Recall: {recall}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"F1 Score: {f1}")
    print("-" * 50)




import matplotlib.pyplot as plt
import numpy as np

# Define lists to store evaluation metrics
model_names = []
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_auc_scores = []

# Loop through each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    predictions = model.predict(X_test)
    
    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted')
    recall = recall_score(y_test, predictions, average='weighted')
    f1 = f1_score(y_test, predictions, average='weighted')
    
    # Check if the model has predict_proba method
    if hasattr(model, 'predict_proba'):
        probabilities = model.predict_proba(X_test)
        if len(probabilities[0]) > 2:
            roc_auc = roc_auc_score(y_test, probabilities, average='weighted', multi_class='ovr')
        else:
            roc_auc = roc_auc_score(y_test, probabilities[:, 1], average='weighted')
    else:
        roc_auc = -1  # Placeholder value for models without ROC AUC
    
    # Append metrics to lists
    model_names.append(model_name)
    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1)
    roc_auc_scores.append(roc_auc)

# Convert lists to arrays for plotting
model_names = np.array(model_names)
accuracies = np.array(accuracies)
precisions = np.array(precisions)
recalls = np.array(recalls)
f1_scores = np.array(f1_scores)
roc_auc_scores = np.array(roc_auc_scores)

# Plotting
fig, axs = plt.subplots(3, 2, figsize=(16, 12))
fig.suptitle('Evaluation Metrics for Different Models', fontsize=16, fontweight='bold')

# Bar width
bar_width = 0.35

# Position of bars on x-axis
r = np.arange(len(model_names))

# Plotting bars for each metric
metrics = [accuracies, precisions, recalls, f1_scores, roc_auc_scores]
metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']

for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
    ax = axs[i // 2, i % 2]
    ax.bar(r, metric, color=plt.cm.tab10(i), width=bar_width, edgecolor='grey', label=metric_name)
    ax.set_xlabel('Model', fontweight='bold', fontsize=12)
    ax.set_ylabel(metric_name, fontweight='bold', fontsize=12)
    ax.set_xticks(r)
    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=10)

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()




from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Define the models dictionary
models = {
    "LGBMClassifier": LGBMClassifier(),
    "AdaBoostClassifier": AdaBoostClassifier(),
    "ExtraTreesClassifier": ExtraTreesClassifier(),
    "NuSVC": NuSVC(probability=True),
    "ExtraTreeClassifier": ExtraTreeClassifier(),
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM": SVC(probability=True),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Loop through each model
for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    predictions = model.predict(X_test)
    
    # Generate confusion matrix
    cm = confusion_matrix(y_test, predictions)
    
    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()




for model_name, model in models.items():
    # Calculate probabilities for the negative class
    y_proba = model.predict_proba(X_test)[:, 0]
    
    # Compute the ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    
    # Calculate True Negative Rate (TNR) and False Negative Rate (FNR)
    tnr = 1 - fpr
    fnr = 1 - tpr
    
    # Compute AUC
    auc = roc_auc_score(y_test, y_proba)
    
    # Plot the inverted ROC curve for each algorithm
    plt.figure(figsize=(8, 6))
    plt.plot(fnr, tnr, label=f'{model_name} (AUC = {auc:.2f})')
    
    # Plot the diagonal line
    plt.plot([0, 1], [0, 1], linestyle='--', color='black')
    
    # Set plot labels, title, and legend
    plt.xlabel('False Negative Rate (FNR)')
    plt.ylabel('True Negative Rate (TNR)')
    plt.title(f'Inverted ROC Curve - {model_name}', weight='bold', size=13)
    plt.legend()
    
    # Remove plot borders
    plt.box(False)
    
    # Show plot
    plt.show()
    











